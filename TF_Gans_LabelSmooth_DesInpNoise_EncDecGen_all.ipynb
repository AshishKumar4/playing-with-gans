{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCSGKZcJDnx7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from keras.utils import to_categorical  # Only for categorical one hot encoding\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "iPFOgjFKEjWV",
        "outputId": "6faa4336-0745-4d47-e494-2c569f80d265"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "cy_train = np.array(to_categorical(y_train))\n",
        "cy_test = np.array(to_categorical(y_test))\n",
        "\n",
        "cx_train, cx_test = np.array((x_train.reshape(x_train.shape[0], 28, 28, 1) - 127.5)/127.5), np.array((x_test.reshape(x_test.shape[0], 28, 28, 1)-127.5)/127.5)\n",
        "cx_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IbcHQAsFiJG"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output) * 0.9, real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    # total_loss = tf.concat((fake_loss, real_loss), axis=0)\n",
        "    total_loss = (fake_loss + real_loss) * 0.5\n",
        "    return total_loss #* 0.5\n",
        "    \n",
        "def wasserstein_discriminator_loss(real_output, fake_output):\n",
        "    total_loss = -tf.reduce_mean(real_output - fake_output)\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output) * 0.9, fake_output)\n",
        "\n",
        "def wasserstein_generator_loss(fake_output):\n",
        "    total_loss = -tf.reduce_mean(fake_output)\n",
        "    return total_loss\n",
        "\n",
        "def generator_enc_loss(real, fake):\n",
        "  # return tf.reduce_mean(tf.abs(real - fake))\n",
        "  return tf.keras.losses.mean_absolute_error(real, fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCvSyhYbL9z0"
      },
      "outputs": [],
      "source": [
        "def generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(512, use_bias=False, input_shape=(128,)))\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (7, 7), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization(momentum=0.8))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (7, 7), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def discriminator():\n",
        "    inputLayer = layers.Input((28, 28, 1))\n",
        "    x = tf.keras.layers.GaussianNoise(0.05)(inputLayer)\n",
        "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
        "    # x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
        "    # x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
        "    # x = layers.Dropout(0.3)(x)\n",
        "    print(x.shape)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    encOut = x\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    dis = tf.keras.models.Model(inputs=inputLayer, outputs=x)\n",
        "    enc = tf.keras.models.Model(inputs=inputLayer, outputs=encOut)\n",
        "\n",
        "    return dis, enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ab6RYkm6Fw92",
        "outputId": "642d1d04-932d-4836-ceed-879d7e2ed83c"
      },
      "outputs": [],
      "source": [
        "gen = generator()\n",
        "des, enc = discriminator()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQgQY8nVE4XY"
      },
      "outputs": [],
      "source": [
        "def trainDesGenEnc(gen, des, enc, real, batch_size):\n",
        "  with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
        "    noise = enc(real, training=False)\n",
        "    fake = gen(noise, training=True)\n",
        "    X = tf.concat((fake, real), axis=0)\n",
        "    pred = des(X, training=True)\n",
        "\n",
        "    fake_output = pred[:batch_size]\n",
        "    real_output = pred[batch_size:]\n",
        "\n",
        "    des_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(des_loss, des.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, des.trainable_variables))\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, gen.trainable_variables))\n",
        "\n",
        "# @tf.function\n",
        "def trainGenEnc(gen, enc, real, batch_size):\n",
        "  with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
        "    real_enc = enc(real, training=False)\n",
        "    enc_fake = gen(real_enc, training=True)\n",
        "\n",
        "    gen_loss = generator_enc_loss(real, enc_fake) * 0.5\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, gen.trainable_variables))\n",
        "\n",
        "# @tf.function\n",
        "def trainDes(gen, des, real, batch_size):\n",
        "  with tf.GradientTape() as disc_tape:\n",
        "    noise = tf.random.normal([batch_size, 128])\n",
        "\n",
        "    fake = gen(noise, training=False)\n",
        "\n",
        "    X = tf.concat((fake, real), axis=0)\n",
        "    \n",
        "    pred = des(X, training=True)\n",
        "\n",
        "    fake_output = pred[:batch_size]\n",
        "    real_output = pred[batch_size:]\n",
        "\n",
        "    des_loss = discriminator_loss(real_output, fake_output)\n",
        "    gradients_of_discriminator = disc_tape.gradient(des_loss, des.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, des.trainable_variables))\n",
        "\n",
        "# @tf.function\n",
        "def trainDesGen(gen, des, real, batch_size):\n",
        "  with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
        "    noise = tf.random.normal([batch_size, 128])\n",
        "    fake = gen(noise, training=True)\n",
        "    X = tf.concat((fake, real), axis=0)\n",
        "    pred = des(X, training=True)\n",
        "\n",
        "    fake_output = pred[:batch_size]\n",
        "    real_output = pred[batch_size:]\n",
        "\n",
        "    des_loss = discriminator_loss(real_output, fake_output)\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(des_loss, des.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, des.trainable_variables))\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, gen.trainable_variables))\n",
        "\n",
        "# @tf.function\n",
        "def trainGen(gen, des, batch_size):\n",
        "  with tf.GradientTape() as gen_tape:\n",
        "    noise = tf.random.normal([batch_size, 128])\n",
        "\n",
        "    fake = gen(noise, training=True)\n",
        "    fake_output = des(fake, training=False)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, gen.trainable_variables))\n",
        "\n",
        "def evalGan(gen, des, data, batches, batch_size):\n",
        "  desAcc = 0\n",
        "  genLoss = 0\n",
        "  for i in range(batches):\n",
        "    real = data[i]\n",
        "    fake = gen.predict(tf.random.normal([batch_size, 128]))\n",
        "    X = tf.concat((fake, real), axis=0)\n",
        "\n",
        "    output = des.predict(X)\n",
        "\n",
        "    real_output = output[batch_size:]\n",
        "    fake_output = output[:batch_size]\n",
        "\n",
        "    labels = tf.reshape(tf.concat((tf.zeros_like(fake_output), tf.ones_like(real_output)), axis=0), [-1])\n",
        "    output = tf.reshape(output, [-1])\n",
        "    \n",
        "    acc = tf.keras.metrics.binary_accuracy(labels, output, threshold=0.5)\n",
        "    desAcc += acc.numpy()\n",
        "    # print(acc)\n",
        "    genLoss += tf.reduce_sum(generator_loss(fake_output)).numpy()\n",
        "  return desAcc / batches, genLoss / batches\n",
        "\n",
        "def trainGan(realData, epochs=10, batch_size=5, loss='mse'):\n",
        "  realData = realData.reshape(tuple([-1, batch_size] + list(realData.shape[1:])))\n",
        "  print(realData.shape)\n",
        "  noise = tf.random.normal([16, 128])\n",
        "  results = []\n",
        "  for epoch in range(epochs):\n",
        "    realData = tf.random.shuffle(realData)\n",
        "    for iter in range(len(realData)):\n",
        "      real = realData[iter]\n",
        "      real = tf.cast(real, tf.float32)\n",
        "\n",
        "      trainDes(gen, des, real, batch_size)\n",
        "      trainGen(gen, des, batch_size)\n",
        "      # trainDesGen(gen, des, real, batch_size)\n",
        "      # trainGenEnc(gen, enc, real, batch_size)\n",
        "      # trainDesGenEnc(gen, des, enc, real, batch_size)\n",
        "\n",
        "    fake = gen.predict(noise)\n",
        "    print(\"Evaluating:\")\n",
        "    desAcc, genLoss = evalGan(gen, des, realData, 10, batch_size)\n",
        "    results.append({'desAcc':desAcc, 'genLoss':genLoss})\n",
        "    print(\"Epoch \", epoch, desAcc, genLoss)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "    for i in range(fake.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(fake[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m87RNQd3rtGA",
        "outputId": "4b9595a1-b557-4350-b358-88ec74c0e414"
      },
      "outputs": [],
      "source": [
        "trainGan(cx_train, epochs=100, batch_size=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8DevXNcc_CT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
